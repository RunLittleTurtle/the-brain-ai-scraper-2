# The Brain CLI - Scraping Commands

This document provides step-by-step instructions for testing the scraping functionality, with expected outputs for each command.

## Scraping Commands

### Basic Scraping

Execute a scraping operation with structured arguments:

```bash
python -m cli.app scrape --url https://example.com --extract price,title
```

**Expected Output:**
```
Structured request:
  â€¢ Target URL: https://example.com
  â€¢ Fields to extract: price, title
  â€¢ Technical requirements: html_parsing

âš™ï¸ Building scraping pipeline with pipeline_builder...
ğŸ”„ [####----------] 20% Analyzing page requirements
ğŸ”„ [########------] 40% Selecting appropriate tools
ğŸ”„ [############--] 60% Determining tool compatibility
ğŸ”„ [################] 80% Configuring tool parameters
ğŸ”„ [####################] 100% Finalizing pipeline

â„¹ï¸ Proposed pipeline:
  â€¢ Pipeline ID: pipe_xxxxxxxx
  â€¢ Tools:
    1. requests (Reason: Required for content fetching)
    2. beautifulsoup4 (Reason: HTML extraction for specified fields)

Does this pipeline look correct? [Y/n]: Y

ğŸ”„ Executing scrape with executor...
ğŸ”„ [####----------] 20% Initializing tools
ğŸ”„ [########------] 40% Fetching content with requests
ğŸ”„ [############--] 60% Processing content with beautifulsoup4
ğŸ”„ [################] 80% Extracting data
ğŸ”„ [####################] 100% Generating results

âœ“ Scraping completed successfully
Run ID: run_xxxxxxxx
Execution time: x.xx seconds

Extracted data:
  â€¢ price: $49.99
  â€¢ title: Sample Product Title
```

### Scraping with JavaScript Support

Execute a scraping operation that requires JavaScript rendering:

```bash
python -m cli.app scrape --url https://amazon.com/some-product --extract price,title --javascript
```

**Expected Output:**
```
Structured request:
  â€¢ Target URL: https://amazon.com/some-product
  â€¢ Fields to extract: price, title
  â€¢ Technical requirements: html_parsing, javascript_rendering

âš™ï¸ Building scraping pipeline with pipeline_builder...
ğŸ”„ [####----------] 20% Analyzing page requirements
ğŸ”„ [########------] 40% Selecting appropriate tools
ğŸ”„ [############--] 60% Determining tool compatibility
ğŸ”„ [################] 80% Configuring tool parameters
ğŸ”„ [####################] 100% Finalizing pipeline

â„¹ï¸ Proposed pipeline:
  â€¢ Pipeline ID: pipe_xxxxxxxx
  â€¢ Tools:
    1. playwright (Reason: JavaScript rendering needed)
    2. beautifulsoup4 (Reason: HTML extraction for specified fields)

Does this pipeline look correct? [Y/n]: Y

ğŸ”„ Executing scrape with executor...
ğŸ”„ [####----------] 20% Initializing tools
ğŸ”„ [########------] 40% Fetching content with playwright
ğŸ”„ [############--] 60% Processing content with beautifulsoup4
ğŸ”„ [################] 80% Extracting data
ğŸ”„ [####################] 100% Generating results

âœ“ Scraping completed successfully
Run ID: run_xxxxxxxx
Execution time: x.xx seconds

Extracted data:
  â€¢ price: $49.99
  â€¢ title: Sample Product Title
```

### Natural Language Scraping

Execute a scraping operation with a natural language description:

```bash
python -m cli.app scrape "Get the price and title from https://example.com"
```

**Expected Output:**
```
Processing request: Get the price and title from https://example.com
Using intent inference to analyze request...

Inferred intent:
  â€¢ Target URL: https://example.com
  â€¢ Fields to extract: price, title
  â€¢ Technical requirements: html_parsing

âš™ï¸ Building scraping pipeline with pipeline_builder...
ğŸ”„ [####----------] 20% Analyzing page requirements
ğŸ”„ [########------] 40% Selecting appropriate tools
ğŸ”„ [############--] 60% Determining tool compatibility
ğŸ”„ [################] 80% Configuring tool parameters
ğŸ”„ [####################] 100% Finalizing pipeline

â„¹ï¸ Proposed pipeline:
  â€¢ Pipeline ID: pipe_xxxxxxxx
  â€¢ Tools:
    1. requests (Reason: Required for content fetching)
    2. beautifulsoup4 (Reason: HTML extraction for specified fields)

Does this pipeline look correct? [Y/n]: Y

ğŸ”„ Executing scrape with executor...
ğŸ”„ [####----------] 20% Initializing tools
ğŸ”„ [########------] 40% Fetching content with requests
ğŸ”„ [############--] 60% Processing content with beautifulsoup4
ğŸ”„ [################] 80% Extracting data
ğŸ”„ [####################] 100% Generating results

âœ“ Scraping completed successfully
Run ID: run_xxxxxxxx
Execution time: x.xx seconds

Extracted data:
  â€¢ price: $49.99
  â€¢ title: Sample Product Title
```

### Verbose Output

Get detailed output with --verbose flag:

```bash
python -m cli.app --verbose scrape --url https://example.com --extract title
```

**Expected Output:**
```
Verbose mode enabled
Structured request:
  â€¢ Target URL: https://example.com
  â€¢ Fields to extract: title
  â€¢ Technical requirements: html_parsing

âš™ï¸ Building scraping pipeline with pipeline_builder...
ğŸ”„ [####----------] 20% Analyzing page requirements
ğŸ”„ [########------] 40% Selecting appropriate tools
ğŸ”„ [############--] 60% Determining tool compatibility
ğŸ”„ [################] 80% Configuring tool parameters
ğŸ”„ [####################] 100% Finalizing pipeline

â„¹ï¸ Proposed pipeline:
  â€¢ Pipeline ID: pipe_xxxxxxxx
  â€¢ Tools:
    1. requests (Reason: Required for content fetching)
      Config: {'timeout': 10, 'headers': {'User-Agent': 'Mozilla/5.0...'}}
    2. beautifulsoup4 (Reason: HTML extraction for specified fields)
      Config: {'parser': 'html.parser'}
      Selectors:
      â€¢ title: title, h1.product-title, h1.title, h1

Does this pipeline look correct? [Y/n]: Y

ğŸ”„ Executing scrape with executor...
ğŸ”„ [####----------] 20% Initializing tools
ğŸ”„ [########------] 40% Fetching content with requests
ğŸ”„ [############--] 60% Processing content with beautifulsoup4
ğŸ”„ [################] 80% Extracting data
ğŸ”„ [####################] 100% Generating results

âœ“ Scraping completed successfully
Run ID: run_xxxxxxxx
Execution time: x.xx seconds

Extracted data:
  â€¢ title: Sample Product Title
```
